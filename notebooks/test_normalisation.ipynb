{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soniasol/Formation_TEI/blob/main/notebooks/test_normalisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test of the Normalization Pipeline"
      ],
      "metadata": {
        "id": "4EkzlZkTN8FM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install all the dependencies."
      ],
      "metadata": {
        "id": "j6b8_n3oOAgt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVyFLcfAJq00"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install fairseq@git+https://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52\n",
        "!pip install sentencepiece sacrebleu hydra-core omegaconf==2.0.5 gdown==4.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the model files.\n",
        "\n",
        "Later, we can specify our own .zip file (after creating a release file)."
      ],
      "metadata": {
        "id": "XhopEMGmOMmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/gabays/32M7131/releases/download/Norm/Normalisation-models.zip\n",
        "!unzip Normalisation-models.zip\n",
        "!mv -f French-normalisation-data-models data-models\n",
        "!rm Normalisation-models.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpdC-W9eOE30",
        "outputId": "43899154-d92e-4695-9210-2ac28e6b0373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-06 09:43:08--  https://github.com/gabays/32M7131/releases/download/Norm/Normalisation-models.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/609944064/426229d6-b699-4f7b-b942-7f27168e037f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231106%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231106T094308Z&X-Amz-Expires=300&X-Amz-Signature=8e373ce61c319e24e03b99f533664b780c1efe54ee6799fe3e33c94e8e54cb5e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=609944064&response-content-disposition=attachment%3B%20filename%3DNormalisation-models.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-11-06 09:43:09--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/609944064/426229d6-b699-4f7b-b942-7f27168e037f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231106%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231106T094308Z&X-Amz-Expires=300&X-Amz-Signature=8e373ce61c319e24e03b99f533664b780c1efe54ee6799fe3e33c94e8e54cb5e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=609944064&response-content-disposition=attachment%3B%20filename%3DNormalisation-models.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1248768034 (1.2G) [application/octet-stream]\n",
            "Saving to: ‘Normalisation-models.zip’\n",
            "\n",
            "Normalisation-model 100%[===================>]   1.16G   235MB/s    in 4.6s    \n",
            "\n",
            "2023-11-06 09:43:13 (258 MB/s) - ‘Normalisation-models.zip’ saved [1248768034/1248768034]\n",
            "\n",
            "Archive:  Normalisation-models.zip\n",
            "  inflating: French-normalisation-data-models/dict_denorm.txt  \n",
            "  inflating: French-normalisation-data-models/utils.py  \n",
            "  inflating: French-normalisation-data-models/structure_files.sh  \n",
            "  inflating: French-normalisation-data-models/align.py  \n",
            "  inflating: French-normalisation-data-models/bpe_joint_1000.model  \n",
            "  inflating: French-normalisation-data-models/dev.trg  \n",
            "  inflating: French-normalisation-data-models/bpe_joint_1000.vocab  \n",
            "  inflating: French-normalisation-data-models/dev.sp.decade.trg  \n",
            "  inflating: French-normalisation-data-models/dev.src  \n",
            "  inflating: French-normalisation-data-models/dev.norm.full.trg  \n",
            "  inflating: French-normalisation-data-models/dict_norm.txt  \n",
            "  inflating: French-normalisation-data-models/test.src  \n",
            "  inflating: French-normalisation-data-models/test.trg  \n",
            "  inflating: French-normalisation-data-models/train.trg  \n",
            "  inflating: French-normalisation-data-models/levenshtein.py  \n",
            "  inflating: French-normalisation-data-models/test.norm.full.trg  \n",
            "  inflating: French-normalisation-data-models/train.src  \n",
            "  inflating: French-normalisation-data-models/lefff-3.4.mlex.gz  \n",
            "  inflating: French-normalisation-data-models/lstm_denorm.pt  \n",
            "  inflating: French-normalisation-data-models/lstm_norm.pt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone our repository."
      ],
      "metadata": {
        "id": "mToMwfnLOVyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/soniasol/test_normalisation.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWIN428FOXbp",
        "outputId": "c13c8df5-c3a4-4135-c896-97c4dcb09cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'test_normalisation' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Move the model files we are going to be using under the `models` folder."
      ],
      "metadata": {
        "id": "NdDXxZnHQVME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv data-models/bpe_joint_1000.model test_normalisation/models/bpe_joint_1000.model\n",
        "!mv data-models/bpe_joint_1000.vocab test_normalisation/models/bpe_joint_1000.vocab\n",
        "!mv data-models/lstm_norm.pt test_normalisation/models/lstm_norm.pt"
      ],
      "metadata": {
        "id": "1JVDmCEFPmMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a54e446-a7a3-442d-fec4-98f632d1feb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'data-models/bpe_joint_1000.model': No such file or directory\n",
            "mv: cannot stat 'data-models/bpe_joint_1000.vocab': No such file or directory\n",
            "mv: cannot stat 'data-models/lstm_norm.pt': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "We can now start by defining a few functions we are going to use. These functions will eventually be moved to a Python file we will save in the repository. The functions will then be imported as any normal Python function (from a library).\n",
        "\n",
        "For instance, if the file will be saved under `$PATH_TO_REPO/utils/file_utils.py`, where `$PATH_TO_REPO` is the path to the repository on the machine we are using, we will be able to import them by:\n",
        "* adding `$PATH_TO_REPO` to the `PYTHONPATH` (either using the terminal or using the Python library `sys`)\n",
        "* using `from utils import file_utils` and then, e.g., to use the `read_file()` function, `file_utils.read_file()`."
      ],
      "metadata": {
        "id": "StPkU0sKHDlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(filename):\n",
        "\n",
        "  \"\"\"\n",
        "    Read a (text) file line by line.\n",
        "  \"\"\"\n",
        "\n",
        "  # list storing all of the lines (strings)\n",
        "  str_list = []\n",
        "\n",
        "  # `fp` is a file pointer (see documentation) to the file `filename`\n",
        "  # `fp` will exist, in this case, in the scope of the `with` statement only (see documentation)\n",
        "  with open(filename) as fp:\n",
        "\n",
        "    # append every line of the (text) file to our list `str_list`\n",
        "    for line in fp:\n",
        "      str_list.append(line.strip())\n",
        "\n",
        "  # return the list\n",
        "  return str_list\n",
        "\n",
        "# -----------------------------\n",
        "\n",
        "def write_file(str_list, path_to_file):\n",
        "\n",
        "  \"\"\"\n",
        "    Write a list of strings `str_list` to a file `path_to_file`.\n",
        "    The `path_to_file` variable must contain the path to the file and the file extension.\n",
        "    For instance, `path_to_file` might be \"/home/user/Desktop/output.txt\"\n",
        "  \"\"\"\n",
        "\n",
        "  # as before, `fp` is a file pointer (see documentation) to the file `filename`\n",
        "  # `fp` will exist, in this case, in the scope of the `with` statement only (see documentation)\n",
        "  with open(path_to_file, 'w') as fp:\n",
        "\n",
        "    # write every string to the file `filename`\n",
        "    for string in str_list:\n",
        "      fp.write(string + '\\n')"
      ],
      "metadata": {
        "id": "iU4SbpKIQ-1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's populate some static variables (i.e., variables written in capital that we know are not going to change and should not be changed throughout the whole notebook!)"
      ],
      "metadata": {
        "id": "r4jkqaa3JUn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "PATH_TO_REPO = \"/content/test_normalisation\"\n",
        "\n",
        "PATH_TO_MODELS = os.path.join(PATH_TO_REPO, \"models\")\n",
        "\n",
        "PATH_TO_DATA = os.path.join(PATH_TO_REPO, \"data\")\n",
        "PATH_TO_INPUT_DATA = os.path.join(PATH_TO_DATA, \"input_data\")\n",
        "PATH_TO_OUTPUT_DATA = os.path.join(PATH_TO_DATA, \"output_data\")"
      ],
      "metadata": {
        "id": "nCTrv-B2JR55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also make sure the output data folder is created (if not there yet!)\n",
        "\n",
        "Note: use `os.mkdir` whenever you are sure only the last directory does not exist. In this case, `/content/test_normalisation/data` SHOULD exists, and if not we want to have an error popping up (that's why `os.mkdir`). Conversely, `os.makedirs` will create the whole directory tree."
      ],
      "metadata": {
        "id": "GOYFiL-xJ_G9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(PATH_TO_OUTPUT_DATA):\n",
        "  print(\"The directory\", PATH_TO_OUTPUT_DATA, \"doesn't exist yet. Creating it...\")\n",
        "  os.mkdir(PATH_TO_OUTPUT_DATA)\n",
        "else:\n",
        "  print(\"The directory\", PATH_TO_OUTPUT_DATA, \"exists already.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAW5y9rCJ-We",
        "outputId": "52bfeae0-e386-4450-f71d-f99920ce2301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The directory /content/test_normalisation/data/output_data exists already.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then test the `read_file()` function right away.\n",
        "\n",
        "The following command uses what's called \"list comprehension\". A list comprehension is when \"something in a list\" is assigned to a variable straight away, like in this case. What's in the list usually follow this syntax: `x for x in whatever() if something in x` where `x` can be whatever letter/variable NOT used by Python by default (e.g., DO NOT use `str` or `file`), `whatever()` is a function returning something, and `something` is a condition we want the element `x` to meet. One can also omit `if something in x`."
      ],
      "metadata": {
        "id": "zUtSpsd8JuAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_files_list = [f for f in os.listdir(PATH_TO_INPUT_DATA) if f.lower().endswith(\".txt\")]"
      ],
      "metadata": {
        "id": "7IqlRO4cJuqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is equivalent to the much longer:\n",
        "\n",
        "```\n",
        "input_files_list = list()\n",
        "\n",
        "# get all the files in the directory\n",
        "for f in os.listdir(PATH_TO_INPUT_DATA):\n",
        "\n",
        "  # keep only the files that end in \".txt\"\n",
        "  if f.lower().endswith(\".txt\"):\n",
        "    input_files_list.append(f)\n",
        "```"
      ],
      "metadata": {
        "id": "EQ5osAUkLfQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's select the first text file, in alphabetical order, from the `PATH_TO_INPUT_DATA` folder."
      ],
      "metadata": {
        "id": "Ca6O3t_qMoA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the list\n",
        "print(\"input_files_list:\", input_files_list)\n",
        "\n",
        "# select the first element/file (fn stands for \"filename\")\n",
        "input_fn = input_files_list[0]\n",
        "print(\"\\ninput_fn:\", input_fn)\n",
        "\n",
        "# generate the path to `input_fn`\n",
        "path_to_input_file = os.path.join(PATH_TO_INPUT_DATA, input_fn)\n",
        "print(\"\\npath_to_input_file:\", path_to_input_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3-ct4FaL866",
        "outputId": "f7388c69-953e-4205-8dfe-6769bf91e7fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_files_list: ['Test0_Chansons_nouvelles.txt', 'Test1_Chansons_nouvelles.txt']\n",
            "\n",
            "input_fn: Test0_Chansons_nouvelles.txt\n",
            "\n",
            "path_to_input_file: /content/test_normalisation/data/input_data/Test0_Chansons_nouvelles.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's read the file."
      ],
      "metadata": {
        "id": "VkQu9gOqMpAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check: the file should exist!\n",
        "assert os.path.exists(path_to_input_file), \"The file %s doesn't exist! Exiting...\"%(path_to_input_file)\n",
        "\n",
        "input_file = read_file(path_to_input_file)"
      ],
      "metadata": {
        "id": "xRTHbdfiMp7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can inspect the input file.\n",
        "\n",
        "Remember this will be a list of strings. We can print the first 5 strings in the list!"
      ],
      "metadata": {
        "id": "S438KYSUNI8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_file[0:5]"
      ],
      "metadata": {
        "id": "tXTw3x53M1H_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "328bfd1b-5ef7-4c86-d51f-8f29a43f3ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '¶ Chanson nouuelle de lorigine/',\n",
              " 'autorite/ ⁊ puissance de leuãgile/',\n",
              " 'contre tous ceulx qui le appellent',\n",
              " 'nouuelle doctrine/ Sur le chant:']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Let's now preprocess the sentences to be normalized by tokenizing them."
      ],
      "metadata": {
        "id": "Jogx13kONWKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece\n",
        "\n",
        "tokenization_model_name = \"bpe_joint_1000.model\"\n",
        "path_to_tokenization_model = os.path.join(PATH_TO_MODELS, tokenization_model_name)\n",
        "\n",
        "# load the model in a variable named `spm`\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file=path_to_tokenization_model)\n",
        "\n",
        "# tokenize the input file!\n",
        "input_file_tokenized = spm.encode(input_file, out_type=str)"
      ],
      "metadata": {
        "id": "pGP87DKfNRzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can inspect the output of the tokenization process by printing a few elements of `input_file_tokenized`. Remember, this will be a LIST OF LISTS (every line of the `.txt` file will generate a list!) with A LOT of elements!\n",
        "\n",
        "For instance, `input_file_tokenized[0]` will be the tokenization of the first line in the `.txt` file, `input_file_tokenized[1]` the second line, and so on."
      ],
      "metadata": {
        "id": "AUtx3HanN5jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_tokenized[1]"
      ],
      "metadata": {
        "id": "_yIJQynmNmWL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edcee92e-c106-46a6-f06a-2c2d9c0e9a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁',\n",
              " '¶',\n",
              " '▁Ch',\n",
              " 'ans',\n",
              " 'on',\n",
              " '▁n',\n",
              " 'ouu',\n",
              " 'elle',\n",
              " '▁de',\n",
              " '▁l',\n",
              " 'or',\n",
              " 'ig',\n",
              " 'ine',\n",
              " '/']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can save the result of the tokenization step in a `.txt` file named as the `.txt` file we tokenized, but with `_tokenized` at the end!\n",
        "\n",
        "(this step is not really mandatory, but it's good for debugging purposes as well!)\n",
        "\n",
        "Since the `write_file()` function expects a single list as an input, we will need to join all the lists in `input_file_tokenized` (which, again, will store a list of lists) into one big list. We can do so in several ways, one of them being the Python function `join()` and list comprehension again."
      ],
      "metadata": {
        "id": "qgigHFWtOfye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentence_list = [' '.join(token) for token in input_file_tokenized]"
      ],
      "metadata": {
        "id": "FUaD9XTxQZrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every sentence will now be tokenized, and every token will be separated by a space. We can verify this by printing one element of the list (string, or line). For instance, `tokenized_sentence_list[1]` will store the tokenized version of `input_file[1]`, where every token is separated by a space."
      ],
      "metadata": {
        "id": "C3a2WyGkQ3Yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input string/line:\", input_file[1])\n",
        "\n",
        "print(\"Tokenized version (space separated):\", tokenized_sentence_list[1])"
      ],
      "metadata": {
        "id": "qhhnFTAJQ0VP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a880d02a-8038-448a-af5e-d8be02fbbb43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input string/line: ¶ Chanson nouuelle de lorigine/\n",
            "Tokenized version (space separated): ▁ ¶ ▁Ch ans on ▁n ouu elle ▁de ▁l or ig ine /\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the name of the input file, without the extension\n",
        "tokenized_file_name, extension = os.path.splitext(os.path.basename(path_to_input_file))\n",
        "\n",
        "# add \"_tokenized\" at the end of the name, and add the extension back as well\n",
        "tokenized_file_name = tokenized_file_name + \"_tokenized\" + extension\n",
        "\n",
        "print(\"tokenized_file_name:\", tokenized_file_name)\n",
        "\n",
        "path_to_tokenized_file = os.path.join(PATH_TO_OUTPUT_DATA, tokenized_file_name)\n",
        "print(\"\\npath_to_tokenized_file:\", path_to_tokenized_file)"
      ],
      "metadata": {
        "id": "0nvnhR83N9r9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c81ad95-8708-44b0-de66-bdaba8ab0ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized_file_name: Test0_Chansons_nouvelles_tokenized.txt\n",
            "\n",
            "path_to_tokenized_file: /content/test_normalisation/data/output_data/Test0_Chansons_nouvelles_tokenized.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the file at `path_to_tokenized_file`."
      ],
      "metadata": {
        "id": "RFkmpeN0QQpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "write_file(tokenized_sentence_list, path_to_tokenized_file)"
      ],
      "metadata": {
        "id": "P1EQwlbSPPEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Finally, we can run the `fairseq` model trained for normalization purposes on the tokens/sentences saved in the file at `path_to_tokenized_file`. Let's break down the following command before running it.\n",
        "\n",
        "```\n",
        "!head -n 10 $INPUT_FILE | fairseq-interactive $PATH_TO_MODELS --source-lang src --target-lang trg --path $PATH_TO_MODEL_FILE > $OUTPUT_FILE\n",
        "```"
      ],
      "metadata": {
        "id": "43RDm8fARb_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command `!head -n 10 $INPUT_FILE ` returns the first 10 elements of the file at `$INPUT_FILE`. This is a Linux command, so we need to either specify the path to the file or use `$` to tell Linux `path_to_tokenized_file` is a variable that stores something and not a simple text string."
      ],
      "metadata": {
        "id": "fYbWHaSXR_pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 10 $path_to_tokenized_file"
      ],
      "metadata": {
        "id": "nuGfPjEoR_xM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d3385ef-96ca-4eff-c1ef-31cbba16d0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "▁ ¶ ▁Ch ans on ▁n ouu elle ▁de ▁l or ig ine /\n",
            "▁au t or ite / ▁ ⁊ ▁puiss ance ▁de ▁l eu ã g ile /\n",
            "▁contre ▁tous ▁c eu l x ▁qui ▁le ▁app el l ent\n",
            "▁n ouu elle ▁d oc tr ine / ▁S ur ▁le ▁ch ant :\n",
            "▁Ie ▁ne ▁s c ay ▁pas ▁c õ ment .\n",
            "▁I\n",
            "▁E ▁mes b a h is ▁c õ ment\n",
            "▁L h um ain ▁ent end ement /\n",
            "▁R emp l y ▁dou tre c uy d ance /\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command `| fairseq-interactive $PATH_TO_DICTS --source-lang src --target-lang trg --path $PATH_TO_MODEL-FILE` runs the model on the sentences outputed by the `head` command (see above).\n",
        "\n",
        "Finally, `> $OUTPUT_FILE` saves the output in `$OUTPUT_FILE`."
      ],
      "metadata": {
        "id": "GkIgS0E5R_2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a variable storing the path to the normalization model."
      ],
      "metadata": {
        "id": "AZdeIRySSvrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalization_model_name = \"lstm_norm.pt\"\n",
        "path_to_normalization_model = os.path.join(PATH_TO_MODELS, normalization_model_name)"
      ],
      "metadata": {
        "id": "oqWzRvCmSuw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also create a variable storing the path to the file we want to save the output of the normalization step in."
      ],
      "metadata": {
        "id": "S8JbEEXrTFQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the name of the input file, without the extension\n",
        "normalized_file_name, extension = os.path.splitext(os.path.basename(path_to_input_file))\n",
        "\n",
        "# add \"_tokenized\" at the end of the name, and add the extension back as well\n",
        "normalized_file_name = normalized_file_name + \"_normalized\" + extension\n",
        "\n",
        "print(\"normalized_file_name:\", normalized_file_name)\n",
        "\n",
        "path_to_normalized_file = os.path.join(PATH_TO_OUTPUT_DATA, normalized_file_name)\n",
        "print(\"\\npath_to_normalized_file:\", path_to_normalized_file)"
      ],
      "metadata": {
        "id": "QQIyJJsnTEwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfbcf669-f5ba-4016-b972-7e8adfc9d499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normalized_file_name: Test0_Chansons_nouvelles_normalized.txt\n",
            "\n",
            "path_to_normalized_file: /content/test_normalisation/data/output_data/Test0_Chansons_nouvelles_normalized.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running the normalization, we need to create two symbolic links for the `fairseq` normalization model to work. These \"symbolic links\" are basically like copies of a file. The file we want to copy is `dict_norm.txt` file. We need to copies for the model to work: one needs to be `dict.src.txt` and the other `dict.trg.txt`, so that when we specify the source and target language the model knows what to work with! The location of the dictionaries must be specified after `fairseq-interactive` before the `--source-lang` and the `--target-lang` flags. In our case, we will save the symbolic links in the folder specified by the variable `PATH_TO_MODELS`.\n",
        "\n",
        "Note: we can populate these in the release of the repository to make this cleaner. No worries!"
      ],
      "metadata": {
        "id": "e6msG9HNU4eJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create symlinks\n",
        "!ln -sf /content/data-models/dict_norm.txt $PATH_TO_MODELS/dict.src.txt\n",
        "!ln -sf /content/data-models/dict_norm.txt $PATH_TO_MODELS/dict.trg.txt"
      ],
      "metadata": {
        "id": "EPATdtDoU7s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's run the normalization step on the first 10 lines of the input `.txt` file. Note that this might take a moment."
      ],
      "metadata": {
        "id": "uYKv7SDhTKKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 10 $path_to_tokenized_file | fairseq-interactive $PATH_TO_MODELS --source-lang src --target-lang trg --path $path_to_normalization_model > $path_to_normalized_file"
      ],
      "metadata": {
        "id": "X8BeGngBRnR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f51fb1-9957-4471-f77a-7832bac01582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-06 09:43:30.149685: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-06 09:43:30.149749: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-06 09:43:30.149774: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-06 09:43:30.155204: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-06 09:43:31.276271: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's inspect some lines of the normalized file by loading it first."
      ],
      "metadata": {
        "id": "XTQ4umI7XTZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_file = read_file(path_to_normalized_file)\n",
        "\n",
        "normalized_file[12:25]"
      ],
      "metadata": {
        "id": "fBDCuFxbTjPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3512a5cc-6117-4d26-8ef6-9332c122450d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['W-1\\t0.220\\tseconds',\n",
              " 'H-1\\t-0.2047189176082611\\t▁ Ô ▁Ch ans on ▁n ouu elle ▁de ▁l or ig ine',\n",
              " 'D-1\\t-0.2047189176082611\\t▁ Ô ▁Ch ans on ▁n ouu elle ▁de ▁l or ig ine',\n",
              " 'P-1\\t-0.0001 -1.7299 -0.0002 -0.3039 -0.0006 -0.0002 -0.2201 -0.0223 -0.0001 -0.0000 -0.0003 -0.0001 -0.0000 -0.5882',\n",
              " 'S-2\\t▁au t or ite / ▁ <unk> ▁puiss ance ▁de ▁l eu ã g ile /',\n",
              " 'W-2\\t0.254\\tseconds',\n",
              " 'H-2\\t-0.41317692399024963\\t▁au t or ite ▁: ▁ coup ▁puiss ance ▁de ▁l eu g ile ile',\n",
              " 'D-2\\t-0.41317692399024963\\t▁au t or ite ▁: ▁ coup ▁puiss ance ▁de ▁l eu g ile ile',\n",
              " 'P-2\\t-0.0445 -0.0000 -0.0000 -0.0003 -3.0227 -0.1079 -3.0655 -0.0003 -0.0006 -0.0002 -0.0008 -0.0306 -0.0889 -0.0021 -0.2465 -0.0000',\n",
              " 'S-3\\t▁contre ▁tous ▁c eu l x ▁qui ▁le ▁app el l ent',\n",
              " 'W-3\\t0.238\\tseconds',\n",
              " 'H-3\\t-0.08134998381137848\\t▁contre ▁tous ▁c eu l x ▁qui ▁le ▁app el ent',\n",
              " 'D-3\\t-0.08134998381137848\\t▁contre ▁tous ▁c eu l x ▁qui ▁le ▁app el ent']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The file contains a lot of information we don't need. Let's define a couple of functions to keep only the normalized sentences and then de-tokenize them."
      ],
      "metadata": {
        "id": "ivRZBrJ8X-7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_hypothesis(path_to_file):\n",
        "    outputs = []\n",
        "    with open(path_to_file) as fp:\n",
        "        for line in fp:\n",
        "            # keep only the lines starting with H- (that stands for hypothesis)\n",
        "            if 'H-' in line:\n",
        "                # keep only the third column (since the indices start from [0], this will be [2])\n",
        "                outputs.append(line.strip().split('\\t')[2])\n",
        "    return outputs\n",
        "\n",
        "# -----------------------------\n",
        "\n",
        "def decode_sp(str_list):\n",
        "    return [''.join(sent).replace(' ', '').replace('▁', ' ').strip() for sent in str_list]"
      ],
      "metadata": {
        "id": "EFsrCEopXhXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's keep only the normalized sentence and de-tokenize it. We can compare it to the sentences in the input file."
      ],
      "metadata": {
        "id": "8w4uX5tnY5f7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_sentences_tokenized = extract_hypothesis(path_to_normalized_file)\n",
        "normalized_sentences = decode_sp(normalized_sentences_tokenized)\n",
        "num_normalized_sentences = len(normalized_sentences)\n",
        "\n",
        "# overwrite `path_to_normalized_file` with the sentences only\n",
        "write_file(normalized_sentences, path_to_normalized_file)\n",
        "\n",
        "for line_number, line in enumerate(zip(input_file[0:num_normalized_sentences], normalized_sentences)):\n",
        "  print(\"\\nLine\", line_number)\n",
        "  print(\"\\torig:\", line[0])\n",
        "  print(\"\\tnorm:\", line[1])"
      ],
      "metadata": {
        "id": "zuDSVkR9Y8Hy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4beea2c-4ab0-4f68-87d2-5e1c62607a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Line 0\n",
            "\torig: \n",
            "\tnorm: Â\n",
            "\n",
            "Line 1\n",
            "\torig: ¶ Chanson nouuelle de lorigine/\n",
            "\tnorm: Ô Chanson nouuelle de lorigine\n",
            "\n",
            "Line 2\n",
            "\torig: autorite/ ⁊ puissance de leuãgile/\n",
            "\tnorm: autorite : coup puissance de leugileile\n",
            "\n",
            "Line 3\n",
            "\torig: contre tous ceulx qui le appellent\n",
            "\tnorm: contre tous ceulx qui le appelent\n",
            "\n",
            "Line 4\n",
            "\torig: nouuelle doctrine/ Sur le chant:\n",
            "\tnorm: nouuelle doctrineine Sur le chant:\n",
            "\n",
            "Line 5\n",
            "\torig: Ie ne scay pas cõment.\n",
            "\tnorm: Je ne sais pas commment.\n",
            "\n",
            "Line 6\n",
            "\torig: I\n",
            "\tnorm: I\n",
            "\n",
            "Line 7\n",
            "\torig: E mesbahis cõment\n",
            "\tnorm: E mesbahis commment\n",
            "\n",
            "Line 8\n",
            "\torig: Lhumain entendement/\n",
            "\tnorm: Lhumain entendement\n",
            "\n",
            "Line 9\n",
            "\torig: Remply doutrecuydance/\n",
            "\tnorm: Remply doutrecuidance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can download the outputs (normalized and tokenized) file(s) running the following cells."
      ],
      "metadata": {
        "id": "4ODZUZcwfzUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(path_to_tokenized_file)"
      ],
      "metadata": {
        "id": "hPeJbWMxadjo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f2ba62f9-ff0b-4546-f47b-7e693a1b307b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_60671f73-6a37-4257-b5f8-b91298b34821\", \"Test0_Chansons_nouvelles_tokenized.txt\", 20835)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(path_to_normalized_file)"
      ],
      "metadata": {
        "id": "rZKd5pcWgXGw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "22d368d6-c70d-49a4-8eca-4aa4adb12611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_655aebae-3241-44b2-a3ad-99c53f84f9f4\", \"Test0_Chansons_nouvelles_normalized.txt\", 233)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ehnfriz6hAew"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}